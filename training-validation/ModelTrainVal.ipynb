{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a21a61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required Libraries\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from data.HousePriceDataset import HousePriceDataset\n",
    "from model.PredictorModel import HousePricePredictor\n",
    "from ModelTrainingEpoch import model_training_epoch\n",
    "from ModelValidationEpoch import model_validation_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "932b030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and Preprocess Data\n",
    "housing_price_dataset = pd.read_csv(\"../data/housing.csv\")\n",
    "housing_price_dataset.columns = housing_price_dataset.columns.str.strip()\n",
    "\n",
    "housing_data_input = housing_price_dataset.drop(columns=[\"median_house_value\", \"ocean_proximity\"])\n",
    "housing_data_output = housing_price_dataset[\"median_house_value\"]\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    housing_price_dataset,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "X_train_raw = train_df.drop(columns=[\"median_house_value\", \"ocean_proximity\"], errors='ignore')\n",
    "Y_train_raw = train_df[\"median_house_value\"]\n",
    "\n",
    "X_test_raw = test_df.drop(columns=[\"median_house_value\", \"ocean_proximity\"], errors='ignore')\n",
    "Y_test_raw = test_df[\"median_house_value\"]\n",
    "\n",
    "# Scaling\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train = scaler_x.fit_transform(X_train_raw.values)\n",
    "Y_train = scaler_y.fit_transform(Y_train_raw.values.reshape(-1, 1))\n",
    "\n",
    "X_test = scaler_x.transform(X_test_raw.values)\n",
    "Y_test = scaler_y.transform(Y_test_raw.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "427e497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Setup DataLoaders\n",
    "train_dataset = HousePriceDataset(X_train, Y_train)\n",
    "val_dataset = HousePriceDataset(X_test, Y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b1d52e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Initialize Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HousePricePredictor(input_dim=X_train.shape[1])\n",
    "loss_fn = MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5c78a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training on cpu...\n",
      "Epoch [1/50] Train Loss: 0.3555 Val Loss: nan\n",
      "Epoch [2/50] Train Loss: 0.2926 Val Loss: nan\n",
      "Epoch [3/50] Train Loss: 0.2756 Val Loss: nan\n",
      "Epoch [4/50] Train Loss: 0.2660 Val Loss: nan\n",
      "Epoch [5/50] Train Loss: 0.2595 Val Loss: nan\n",
      "Epoch [6/50] Train Loss: 0.2501 Val Loss: nan\n",
      "Epoch [7/50] Train Loss: 0.2430 Val Loss: nan\n",
      "Epoch [8/50] Train Loss: 0.2394 Val Loss: nan\n",
      "Epoch [9/50] Train Loss: 0.2349 Val Loss: nan\n",
      "Epoch [10/50] Train Loss: 0.2319 Val Loss: nan\n",
      "Epoch [11/50] Train Loss: 0.2300 Val Loss: nan\n",
      "Epoch [12/50] Train Loss: 0.2259 Val Loss: nan\n",
      "Epoch [13/50] Train Loss: 0.2238 Val Loss: nan\n",
      "Epoch [14/50] Train Loss: 0.2227 Val Loss: nan\n",
      "Epoch [15/50] Train Loss: 0.2217 Val Loss: nan\n",
      "Epoch [16/50] Train Loss: 0.2197 Val Loss: nan\n",
      "Epoch [17/50] Train Loss: 0.2173 Val Loss: nan\n",
      "Epoch [18/50] Train Loss: 0.2168 Val Loss: nan\n",
      "Epoch [19/50] Train Loss: 0.2135 Val Loss: nan\n",
      "Epoch [20/50] Train Loss: 0.2140 Val Loss: nan\n",
      "Epoch [21/50] Train Loss: 0.2130 Val Loss: nan\n",
      "Epoch [22/50] Train Loss: 0.2117 Val Loss: nan\n",
      "Epoch [23/50] Train Loss: 0.2101 Val Loss: nan\n",
      "Epoch [24/50] Train Loss: 0.2100 Val Loss: nan\n",
      "Epoch [25/50] Train Loss: 0.2075 Val Loss: nan\n",
      "Epoch [26/50] Train Loss: 0.2082 Val Loss: nan\n",
      "Epoch [27/50] Train Loss: 0.2065 Val Loss: nan\n",
      "Epoch [28/50] Train Loss: 0.2060 Val Loss: nan\n",
      "Epoch [29/50] Train Loss: 0.2060 Val Loss: nan\n",
      "Epoch [30/50] Train Loss: 0.2048 Val Loss: nan\n",
      "Epoch [31/50] Train Loss: 0.2049 Val Loss: nan\n",
      "Epoch [32/50] Train Loss: 0.2019 Val Loss: nan\n",
      "Epoch [33/50] Train Loss: 0.2032 Val Loss: nan\n",
      "Epoch [34/50] Train Loss: 0.2027 Val Loss: nan\n",
      "Epoch [35/50] Train Loss: 0.2010 Val Loss: nan\n",
      "Epoch [36/50] Train Loss: 0.2010 Val Loss: nan\n",
      "Epoch [37/50] Train Loss: 0.2004 Val Loss: nan\n",
      "Epoch [38/50] Train Loss: 0.1993 Val Loss: nan\n",
      "Epoch [39/50] Train Loss: 0.1995 Val Loss: nan\n",
      "Epoch [40/50] Train Loss: 0.1990 Val Loss: nan\n",
      "Epoch [41/50] Train Loss: 0.1980 Val Loss: nan\n",
      "Epoch [42/50] Train Loss: 0.1986 Val Loss: nan\n",
      "Epoch [43/50] Train Loss: 0.1984 Val Loss: nan\n",
      "Epoch [44/50] Train Loss: 0.1980 Val Loss: nan\n",
      "Epoch [45/50] Train Loss: 0.1968 Val Loss: nan\n",
      "Epoch [46/50] Train Loss: 0.1966 Val Loss: nan\n",
      "Epoch [47/50] Train Loss: 0.1967 Val Loss: nan\n",
      "Epoch [48/50] Train Loss: 0.1967 Val Loss: nan\n",
      "Epoch [49/50] Train Loss: 0.1964 Val Loss: nan\n",
      "Epoch [50/50] Train Loss: 0.1947 Val Loss: nan\n"
     ]
    }
   ],
   "source": [
    "# 4. Training Loop\n",
    "num_epochs = 50\n",
    "print(f\"Starting model training on {device}...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = model_training_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    val_loss = model_validation_epoch(\n",
    "        model,\n",
    "        val_loader,\n",
    "        loss_fn,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "        f\"Train Loss: {train_loss:.4f} \"\n",
    "        f\"Val Loss: {val_loss:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326537b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Export the trained model for future\n",
    "import joblib\n",
    "\n",
    "# Assuming 'scaler' is your fitted StandardScaler\n",
    "scaler_path = \"scaler.joblib\"\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved to {scaler_path}\")\n",
    "# Define path\n",
    "model_path = \"inference/model_parameters.pth\"\n",
    "\n",
    "# Save the state_dict (model weights and biases [parameters])\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
