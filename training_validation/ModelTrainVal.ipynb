{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a21a61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required Libraries\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from data.HousePriceDataset import HousePriceDataset\n",
    "from model.PredictorModel import HousePricePredictor\n",
    "from ModelTrainingEpoch import model_training_epoch\n",
    "from ModelValidationEpoch import model_validation_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "932b030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and Preprocess Data\n",
    "housing_price_dataset = pd.read_csv(\"../data/housing.csv\")\n",
    "housing_price_dataset.columns = housing_price_dataset.columns.str.strip()\n",
    "\n",
    "housing_data_input = housing_price_dataset.drop(columns=[\"median_house_value\", \"ocean_proximity\"])\n",
    "housing_data_output = housing_price_dataset[\"median_house_value\"]\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    housing_price_dataset,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "X_train_raw = train_df.drop(columns=[\"median_house_value\", \"ocean_proximity\"], errors='ignore')\n",
    "Y_train_raw = train_df[\"median_house_value\"]\n",
    "\n",
    "X_test_raw = test_df.drop(columns=[\"median_house_value\", \"ocean_proximity\"], errors='ignore')\n",
    "Y_test_raw = test_df[\"median_house_value\"]\n",
    "\n",
    "# Scaling\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train = scaler_x.fit_transform(X_train_raw.values)\n",
    "Y_train = scaler_y.fit_transform(Y_train_raw.values.reshape(-1, 1))\n",
    "\n",
    "X_test = scaler_x.transform(X_test_raw.values)\n",
    "Y_test = scaler_y.transform(Y_test_raw.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "427e497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Setup DataLoaders\n",
    "train_dataset = HousePriceDataset(X_train, Y_train)\n",
    "val_dataset = HousePriceDataset(X_test, Y_test)\n",
    "\n",
    "# Higher batch size for efficient parallel compute training (cuda) on gpu\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1d52e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Initialize Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
    "model_inp = X_train.shape[1]\n",
    "model = HousePricePredictor(input_dim=model_inp)\n",
    "model.to(device)\n",
    "loss_fn = MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5c78a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training on cuda...\n",
      "Epoch [1/50] Train Loss: 0.4713 Val Loss: nan\n",
      "Epoch [2/50] Train Loss: 0.3334 Val Loss: nan\n",
      "Epoch [3/50] Train Loss: 0.3151 Val Loss: nan\n",
      "Epoch [4/50] Train Loss: 0.3012 Val Loss: nan\n",
      "Epoch [5/50] Train Loss: 0.2929 Val Loss: nan\n",
      "Epoch [6/50] Train Loss: 0.2865 Val Loss: nan\n",
      "Epoch [7/50] Train Loss: 0.2805 Val Loss: nan\n",
      "Epoch [8/50] Train Loss: 0.2764 Val Loss: nan\n",
      "Epoch [9/50] Train Loss: 0.2725 Val Loss: nan\n",
      "Epoch [10/50] Train Loss: 0.2698 Val Loss: nan\n",
      "Epoch [11/50] Train Loss: 0.2671 Val Loss: nan\n",
      "Epoch [12/50] Train Loss: 0.2645 Val Loss: nan\n",
      "Epoch [13/50] Train Loss: 0.2622 Val Loss: nan\n",
      "Epoch [14/50] Train Loss: 0.2615 Val Loss: nan\n",
      "Epoch [15/50] Train Loss: 0.2590 Val Loss: nan\n",
      "Epoch [16/50] Train Loss: 0.2557 Val Loss: nan\n",
      "Epoch [17/50] Train Loss: 0.2532 Val Loss: nan\n",
      "Epoch [18/50] Train Loss: 0.2506 Val Loss: nan\n",
      "Epoch [19/50] Train Loss: 0.2476 Val Loss: nan\n",
      "Epoch [20/50] Train Loss: 0.2451 Val Loss: nan\n",
      "Epoch [21/50] Train Loss: 0.2442 Val Loss: nan\n",
      "Epoch [22/50] Train Loss: 0.2423 Val Loss: nan\n",
      "Epoch [23/50] Train Loss: 0.2398 Val Loss: nan\n",
      "Epoch [24/50] Train Loss: 0.2392 Val Loss: nan\n",
      "Epoch [25/50] Train Loss: 0.2380 Val Loss: nan\n",
      "Epoch [26/50] Train Loss: 0.2364 Val Loss: nan\n",
      "Epoch [27/50] Train Loss: 0.2359 Val Loss: nan\n",
      "Epoch [28/50] Train Loss: 0.2337 Val Loss: nan\n",
      "Epoch [29/50] Train Loss: 0.2320 Val Loss: nan\n",
      "Epoch [30/50] Train Loss: 0.2317 Val Loss: nan\n",
      "Epoch [31/50] Train Loss: 0.2289 Val Loss: nan\n",
      "Epoch [32/50] Train Loss: 0.2292 Val Loss: nan\n",
      "Epoch [33/50] Train Loss: 0.2270 Val Loss: nan\n",
      "Epoch [34/50] Train Loss: 0.2264 Val Loss: nan\n",
      "Epoch [35/50] Train Loss: 0.2252 Val Loss: nan\n",
      "Epoch [36/50] Train Loss: 0.2233 Val Loss: nan\n",
      "Epoch [37/50] Train Loss: 0.2245 Val Loss: nan\n",
      "Epoch [38/50] Train Loss: 0.2231 Val Loss: nan\n",
      "Epoch [39/50] Train Loss: 0.2211 Val Loss: nan\n",
      "Epoch [40/50] Train Loss: 0.2212 Val Loss: nan\n",
      "Epoch [41/50] Train Loss: 0.2207 Val Loss: nan\n",
      "Epoch [42/50] Train Loss: 0.2202 Val Loss: nan\n",
      "Epoch [43/50] Train Loss: 0.2190 Val Loss: nan\n",
      "Epoch [44/50] Train Loss: 0.2190 Val Loss: nan\n",
      "Epoch [45/50] Train Loss: 0.2174 Val Loss: nan\n",
      "Epoch [46/50] Train Loss: 0.2174 Val Loss: nan\n",
      "Epoch [47/50] Train Loss: 0.2155 Val Loss: nan\n",
      "Epoch [48/50] Train Loss: 0.2167 Val Loss: nan\n",
      "Epoch [49/50] Train Loss: 0.2143 Val Loss: nan\n",
      "Epoch [50/50] Train Loss: 0.2140 Val Loss: nan\n"
     ]
    }
   ],
   "source": [
    "# 4. Training Loop\n",
    "num_epochs = 50\n",
    "print(f\"Starting model training on {device}...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = model_training_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    val_loss = model_validation_epoch(\n",
    "        model,\n",
    "        val_loader,\n",
    "        loss_fn,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "        f\"Train Loss: {train_loss:.4f} \"\n",
    "        f\"Val Loss: {val_loss:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "326537b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler X saved to ../inference/scaler_x.joblib\n",
      "Scaler Y saved to ../inference/scaler_y.joblib\n",
      "Model saved to ../inference/model_parameters.pth\n",
      "Model input dimension saved to ../inference/model_parameters.pth\n"
     ]
    }
   ],
   "source": [
    "# 5. Export the trained model for future\n",
    "import joblib\n",
    "\n",
    "# Path where fitted StandardScaler from scikit learn is saved\n",
    "scaler_x_path = \"../inference/scaler_x.joblib\"\n",
    "scaler_y_path = \"../inference/scaler_y.joblib\"\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler_x, scaler_x_path)\n",
    "joblib.dump(scaler_y, scaler_y_path)\n",
    "print(f\"Scaler X saved to {scaler_x_path}\")\n",
    "print(f\"Scaler Y saved to {scaler_y_path}\")\n",
    "\n",
    "# Define path\n",
    "model_path = \"../inference/model_parameters.pth\"\n",
    "\n",
    "# Save the state_dict (model weights and biases [parameters])\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Save the model input configuration\n",
    "with open(\"../inference/model_config.py\", \"w\") as config:\n",
    "    config.write(f\"MODEL_INPUT_DIM = {model_inp}\")\n",
    "    print(f\"Model input dimension saved to {model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
