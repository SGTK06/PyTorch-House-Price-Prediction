{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a21a61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required Libraries\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from data.HousePriceDataset import HousePriceDataset\n",
    "from model.PredictorModel import HousePricePredictor\n",
    "from ModelTrainingEpoch import model_training_epoch\n",
    "from ModelValidationEpoch import model_validation_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "932b030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and Preprocess Data\n",
    "housing_price_dataset = pd.read_csv(\"../data/housing.csv\")\n",
    "housing_price_dataset.columns = housing_price_dataset.columns.str.strip()\n",
    "\n",
    "housing_data_input = housing_price_dataset.drop(columns=[\"median_house_value\", \"ocean_proximity\"])\n",
    "housing_data_output = housing_price_dataset[\"median_house_value\"]\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    housing_price_dataset,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "X_train_raw = train_df.drop(columns=[\"median_house_value\", \"ocean_proximity\"], errors='ignore')\n",
    "Y_train_raw = train_df[\"median_house_value\"]\n",
    "\n",
    "X_test_raw = test_df.drop(columns=[\"median_house_value\", \"ocean_proximity\"], errors='ignore')\n",
    "Y_test_raw = test_df[\"median_house_value\"]\n",
    "\n",
    "# Scaling\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train = scaler_x.fit_transform(X_train_raw.values)\n",
    "Y_train = scaler_y.fit_transform(Y_train_raw.values.reshape(-1, 1))\n",
    "\n",
    "X_test = scaler_x.transform(X_test_raw.values)\n",
    "Y_test = scaler_y.transform(Y_test_raw.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427e497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Setup DataLoaders\n",
    "train_dataset = HousePriceDataset(X_train, Y_train)\n",
    "val_dataset = HousePriceDataset(X_test, Y_test)\n",
    "\n",
    "# Higher batch size for efficient parallel compute training (cuda) on gpu\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b1d52e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Initialize Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
    "model_inp = X_train.shape[1]\n",
    "model = HousePricePredictor(input_dim=model_inp)\n",
    "model.to(device)\n",
    "loss_fn = MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5c78a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training on cuda...\n",
      "Epoch [1/50] Train Loss: 0.6517 Val Loss: nan\n",
      "Epoch [2/50] Train Loss: 0.4091 Val Loss: nan\n",
      "Epoch [3/50] Train Loss: 0.3780 Val Loss: nan\n",
      "Epoch [4/50] Train Loss: 0.3435 Val Loss: nan\n",
      "Epoch [5/50] Train Loss: 0.3329 Val Loss: nan\n",
      "Epoch [6/50] Train Loss: 0.3332 Val Loss: nan\n",
      "Epoch [7/50] Train Loss: 0.3196 Val Loss: nan\n",
      "Epoch [8/50] Train Loss: 0.3173 Val Loss: nan\n",
      "Epoch [9/50] Train Loss: 0.3076 Val Loss: nan\n",
      "Epoch [10/50] Train Loss: 0.2990 Val Loss: nan\n",
      "Epoch [11/50] Train Loss: 0.2983 Val Loss: nan\n",
      "Epoch [12/50] Train Loss: 0.2929 Val Loss: nan\n",
      "Epoch [13/50] Train Loss: 0.2891 Val Loss: nan\n",
      "Epoch [14/50] Train Loss: 0.2883 Val Loss: nan\n",
      "Epoch [15/50] Train Loss: 0.2855 Val Loss: nan\n",
      "Epoch [16/50] Train Loss: 0.2789 Val Loss: nan\n",
      "Epoch [17/50] Train Loss: 0.2792 Val Loss: nan\n",
      "Epoch [18/50] Train Loss: 0.2803 Val Loss: nan\n",
      "Epoch [19/50] Train Loss: 0.2743 Val Loss: nan\n",
      "Epoch [20/50] Train Loss: 0.2721 Val Loss: nan\n",
      "Epoch [21/50] Train Loss: 0.2725 Val Loss: nan\n",
      "Epoch [22/50] Train Loss: 0.2731 Val Loss: nan\n",
      "Epoch [23/50] Train Loss: 0.2714 Val Loss: nan\n",
      "Epoch [24/50] Train Loss: 0.2664 Val Loss: nan\n",
      "Epoch [25/50] Train Loss: 0.2668 Val Loss: nan\n",
      "Epoch [26/50] Train Loss: 0.2651 Val Loss: nan\n",
      "Epoch [27/50] Train Loss: 0.2632 Val Loss: nan\n",
      "Epoch [28/50] Train Loss: 0.2616 Val Loss: nan\n",
      "Epoch [29/50] Train Loss: 0.2687 Val Loss: nan\n",
      "Epoch [30/50] Train Loss: 0.2640 Val Loss: nan\n",
      "Epoch [31/50] Train Loss: 0.2599 Val Loss: nan\n",
      "Epoch [32/50] Train Loss: 0.2652 Val Loss: nan\n",
      "Epoch [33/50] Train Loss: 0.2569 Val Loss: nan\n",
      "Epoch [34/50] Train Loss: 0.2570 Val Loss: nan\n",
      "Epoch [35/50] Train Loss: 0.2596 Val Loss: nan\n",
      "Epoch [36/50] Train Loss: 0.2562 Val Loss: nan\n",
      "Epoch [37/50] Train Loss: 0.2592 Val Loss: nan\n",
      "Epoch [38/50] Train Loss: 0.2498 Val Loss: nan\n",
      "Epoch [39/50] Train Loss: 0.2517 Val Loss: nan\n",
      "Epoch [40/50] Train Loss: 0.2525 Val Loss: nan\n",
      "Epoch [41/50] Train Loss: 0.2508 Val Loss: nan\n",
      "Epoch [42/50] Train Loss: 0.2463 Val Loss: nan\n",
      "Epoch [43/50] Train Loss: 0.2520 Val Loss: nan\n",
      "Epoch [44/50] Train Loss: 0.2485 Val Loss: nan\n",
      "Epoch [45/50] Train Loss: 0.2439 Val Loss: nan\n",
      "Epoch [46/50] Train Loss: 0.2581 Val Loss: nan\n",
      "Epoch [47/50] Train Loss: 0.2474 Val Loss: nan\n",
      "Epoch [48/50] Train Loss: 0.2420 Val Loss: nan\n",
      "Epoch [49/50] Train Loss: 0.2382 Val Loss: nan\n",
      "Epoch [50/50] Train Loss: 0.2451 Val Loss: nan\n"
     ]
    }
   ],
   "source": [
    "# 4. Training Loop\n",
    "num_epochs = 50\n",
    "print(f\"Starting model training on {device}...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = model_training_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    val_loss = model_validation_epoch(\n",
    "        model,\n",
    "        val_loader,\n",
    "        loss_fn,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "        f\"Train Loss: {train_loss:.4f} \"\n",
    "        f\"Val Loss: {val_loss:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "326537b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler X saved to ../inference/scaler_x.joblib\n",
      "Scaler Y saved to ../inference/scaler_y.joblib\n",
      "Model saved to ../inference/model_parameters.pth\n"
     ]
    }
   ],
   "source": [
    "# 5. Export the trained model for future\n",
    "import joblib\n",
    "\n",
    "# Path where fitted StandardScaler from scikit learn is saved\n",
    "scaler_x_path = \"../inference/scaler_x.joblib\"\n",
    "scaler_y_path = \"../inference/scaler_y.joblib\"\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler_x, scaler_x_path)\n",
    "joblib.dump(scaler_y, scaler_y_path)\n",
    "print(f\"Scaler X saved to {scaler_x_path}\")\n",
    "print(f\"Scaler Y saved to {scaler_y_path}\")\n",
    "\n",
    "# Define path\n",
    "model_path = \"../inference/model_parameters.pth\"\n",
    "\n",
    "# Save the state_dict (model weights and biases [parameters])\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Save the model input configuration\n",
    "with open(\"../inference/model_config.py\", \"w\") as config:\n",
    "    config.write(f\"MODEL_INPUT_DIM = {model_inp}\")\n",
    "    print(f\"Model input dimension saved to {model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
